{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1061,
     "status": "ok",
     "timestamp": 1728883793127,
     "user": {
      "displayName": "Mix Master",
      "userId": "09511533599127285786"
     },
     "user_tz": -330
    },
    "id": "1s4oP4ZA92CE",
    "outputId": "61c028ed-022a-47b4-f7d9-3271e454cc34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        text sentiment\n",
      "0         I love this product! It's amazing.  Positive\n",
      "1  This is the worst service I've ever used.  Negative\n",
      "2        Had a great time at the park today!  Positive\n",
      "3     Totally disappointed with the quality.  Negative\n",
      "4    Best experience ever, highly recommend!  Positive\n",
      "5                       Not worth the money.  Negative\n",
      "6                Fantastic customer support!  Positive\n",
      "7            Terrible, will never come back.  Negative\n",
      "8          Enjoyed every moment of the trip.  Positive\n",
      "9                         Awful, just awful.  Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Sample data as a string\n",
    "data = \"\"\"\n",
    "text,sentiment\n",
    "\"I love this product! It's amazing.\",Positive\n",
    "\"This is the worst service I've ever used.\",Negative\n",
    "\"Had a great time at the park today!\",Positive\n",
    "\"Totally disappointed with the quality.\",Negative\n",
    "\"Best experience ever, highly recommend!\",Positive\n",
    "\"Not worth the money.\",Negative\n",
    "\"Fantastic customer support!\",Positive\n",
    "\"Terrible, will never come back.\",Negative\n",
    "\"Enjoyed every moment of the trip.\",Positive\n",
    "\"Awful, just awful.\",Negative\n",
    "\"\"\"\n",
    "\n",
    "# Convert the string data to a pandas DataFrame\n",
    "df = pd.read_csv(StringIO(data))\n",
    "df.to_csv('Sentiment.csv', index=False)\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dWhsnpOIx_U_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\manoj\\AppData\\Local\\Temp\\ipykernel_12356\\4146121371.py:17: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  data['text'] = data['text'].apply(lambda x: re.sub('[^a-zA-Z0-9\\s]', '', x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manoj\\AppData\\Local\\Temp\\ipykernel_12356\\4146121371.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[0] = row[0].replace('rt', ' ')\n",
      "C:\\Users\\manoj\\AppData\\Local\\Temp\\ipykernel_12356\\4146121371.py:23: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  row[0] = row[0].replace('rt', ' ')\n",
      "c:\\Users\\manoj\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[ 0  0  0  0  0 44 10 44]]\n",
      "1/1 - 0s - 357ms/step\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('./Sentiment.csv')\n",
    "data = data[['text', 'sentiment']]\n",
    "data = data[data.sentiment != \"Neutral\"]\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply(lambda x: re.sub('[^a-zA-Z0-9\\s]', '', x))\n",
    "\n",
    "print(data[data['sentiment'] == 'Positive'].size)\n",
    "print(data[data['sentiment'] == 'Negative'].size)\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt', ' ')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Model parameters\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Example tweet\n",
    "twt = ['Meetings: Because none of us is as dumb as all of us.']\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "twt = pad_sequences(twt, maxlen=X.shape[1], dtype='int32', value=0)\n",
    "print(twt)\n",
    "\n",
    "# Predict sentiment\n",
    "sentiment = model.predict(twt, batch_size=1, verbose=2)[0]\n",
    "if np.argmax(sentiment) == 0:\n",
    "    print(\"negative\")\n",
    "elif np.argmax(sentiment) == 1:\n",
    "    print(\"positive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fuuvD-Ly76R"
   },
   "outputs": [],
   "source": [
    "#pip install preprocessing tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVGz3b7U7aj-"
   },
   "outputs": [],
   "source": [
    "#!pip uninstall tokenizer\n",
    "#!pip uninstall preprocessing -y\n",
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLrUS_fZ8Hl-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # Use tensorflow.keras for newer versions\n",
    "\n",
    "# Sample text data\n",
    "texts = [\"I love machine learning\", \"Deep learning is amazing\"]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=1000)  # num_words is the maximum number of words to keep, based on word frequency\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print(sequences)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNVIWDz5yfOeMPPLpBGat2J",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
